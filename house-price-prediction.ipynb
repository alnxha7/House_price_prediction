{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd28b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6215eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('houseprice.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38c9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('houseprice.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29467cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6eafd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bd8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   object \n",
      " 5   LotShape     1201 non-null   object \n",
      " 6   YearBuilt    1201 non-null   int64  \n",
      " 7   1stFlrSF     1201 non-null   int64  \n",
      " 8   2ndFlrSF     1201 non-null   int64  \n",
      " 9   SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca63265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>57.198168</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>9951.698585</td>\n",
       "      <td>1970.580350</td>\n",
       "      <td>1158.437968</td>\n",
       "      <td>346.073272</td>\n",
       "      <td>180770.480433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.106427</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>7924.353975</td>\n",
       "      <td>31.750335</td>\n",
       "      <td>386.257235</td>\n",
       "      <td>435.143451</td>\n",
       "      <td>83389.519866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7420.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>876.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9262.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1082.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>159500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11249.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>1383.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>213500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4692.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea    YearBuilt     1stFlrSF  \\\n",
       "count  1201.000000  1201.000000    1201.000000  1201.000000  1201.000000   \n",
       "mean     57.198168    70.049958    9951.698585  1970.580350  1158.437968   \n",
       "std      43.106427    24.284752    7924.353975    31.750335   386.257235   \n",
       "min      20.000000    21.000000    1300.000000  1872.000000   334.000000   \n",
       "25%      20.000000    59.000000    7420.000000  1950.000000   876.000000   \n",
       "50%      50.000000    69.000000    9262.000000  1972.000000  1082.000000   \n",
       "75%      70.000000    80.000000   11249.000000  2003.000000  1383.000000   \n",
       "max     190.000000   313.000000  215245.000000  2010.000000  4692.000000   \n",
       "\n",
       "          2ndFlrSF      SalePrice  \n",
       "count  1201.000000    1201.000000  \n",
       "mean    346.073272  180770.480433  \n",
       "std     435.143451   83389.519866  \n",
       "min       0.000000   34900.000000  \n",
       "25%       0.000000  127500.000000  \n",
       "50%       0.000000  159500.000000  \n",
       "75%     728.000000  213500.000000  \n",
       "max    2065.000000  755000.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98a2ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column MSSubClass has 15 values\n",
      "column MSZoning has 5 values\n",
      "column LotFrontage has 110 values\n",
      "column LotArea has 869 values\n",
      "column Street has 2 values\n",
      "column LotShape has 4 values\n",
      "column YearBuilt has 112 values\n",
      "column 1stFlrSF has 678 values\n",
      "column 2ndFlrSF has 368 values\n",
      "column SalePrice has 597 values\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print('column {} has {} values'.format(i, len(df[i].unique())))\n",
    "    \n",
    "    # here the mmzoning, street, lotshape are categorical value and others are continues values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5040a6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f753ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns= {'YearBuilt':'Total Years'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f90da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Years\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Years\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Years\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 7"
     ]
    }
   ],
   "source": [
    "for i in range(len(df['Total Years'])):\n",
    "    df['Total Years'][i] = datetime.datetime.now().year - df['Total Years'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ffedc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>Total Years</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>21</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>48</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>23</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>109</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>24</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  Total Years  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg           21   \n",
       "1          20       RL         80.0     9600   Pave      Reg           48   \n",
       "2          60       RL         68.0    11250   Pave      IR1           23   \n",
       "3          70       RL         60.0     9550   Pave      IR1          109   \n",
       "4          60       RL         84.0    14260   Pave      IR1           24   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101b1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating categorical features\n",
    "\n",
    "cat_features = ['MSSubClass', 'MSZoning', 'Street', 'LotShape']\n",
    "out_features = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb831c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders = {}\n",
    "lbl_encoders['MSSubClass'] = LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4565d982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSSubClass': LabelEncoder()}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e27440",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoders = {}\n",
    "\n",
    "for features in cat_features:\n",
    "    lbl_encoders[features]=LabelEncoder()\n",
    "    df[features] = lbl_encoders[features].fit_transform(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525154ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>Total Years</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1978</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1941</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1950</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1965</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  \\\n",
       "0              5         3         65.0     8450       1         3   \n",
       "1              0         3         80.0     9600       1         3   \n",
       "2              5         3         68.0    11250       1         0   \n",
       "3              6         3         60.0     9550       1         0   \n",
       "4              5         3         84.0    14260       1         0   \n",
       "...          ...       ...          ...      ...     ...       ...   \n",
       "1455           5         3         62.0     7917       1         3   \n",
       "1456           0         3         85.0    13175       1         3   \n",
       "1457           6         3         66.0     9042       1         3   \n",
       "1458           0         3         68.0     9717       1         3   \n",
       "1459           0         3         75.0     9937       1         3   \n",
       "\n",
       "      Total Years  1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0              21       856       854     208500  \n",
       "1              48      1262         0     181500  \n",
       "2              23       920       866     223500  \n",
       "3             109       961       756     140000  \n",
       "4              24      1145      1053     250000  \n",
       "...           ...       ...       ...        ...  \n",
       "1455         1999       953       694     175000  \n",
       "1456         1978      2073         0     210000  \n",
       "1457         1941      1188      1152     266500  \n",
       "1458         1950      1078         0     142125  \n",
       "1459         1965      1256         0     147500  \n",
       "\n",
       "[1201 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae6e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### stacking and converting in tensors\n",
    "# _____ VALUES ----> NUMPY ----> TORCH ----> TENSORS _________\n",
    "cat_features = np.stack([df['MSSubClass'], df['MSZoning'], df['Street'], df['LotShape']], axis= 1)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898e8c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### converting numpy into tensors\n",
    "# ------- NEVER CHANGE DTYPE OF CATEGORICAL VALUES INTO FLOAT -------- ######\n",
    "\n",
    "cat_features = torch.tensor(cat_features, dtype= torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc5fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = []\n",
    "for i in df.columns:\n",
    "    if i in ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'SalePrice']:\n",
    "        pass\n",
    "    else:\n",
    "        cont_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43957a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', 'Total Years', '1stFlrSF', '2ndFlrSF']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "911f46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____ CONT-VALUES ----> NUMPY ----> TORCH ----> TENSORS ________\n",
    "# stacking it!\n",
    "\n",
    "cont_values = np.stack([df[i].values for i in cont_features], axis= 1)\n",
    "cont_values = torch.tensor(cont_values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a50bcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,    21.,   856.,   854.],\n",
       "        [   80.,  9600.,    48.,  1262.,     0.],\n",
       "        [   68., 11250.,    23.,   920.,   866.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1941.,  1188.,  1152.],\n",
       "        [   68.,  9717.,  1950.,  1078.,     0.],\n",
       "        [   75.,  9937.,  1965.,  1256.,     0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "948bb556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad09070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depentent feature\n",
    "y = torch.tensor(df['SalePrice'].values, dtype= torch.float).reshape(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c86038",
   "metadata": {},
   "source": [
    "#### embedding layers is only for categorical features\n",
    "## embedding size for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06be49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dims = [len(df[col].unique()) for col in ['MSSubClass', 'MSZoning', 'Street', 'LotShape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d70ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims\n",
    "# input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60790e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --thumb rule is that output dimension is based on the input dimension(min(50, unique values / 2))\n",
    "\n",
    "embedding_dim = [(x, min(50, (x + 1) // 2)) for x in cat_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ef0dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5db3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9d07b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_representation = nn.ModuleList([nn.Embedding(inp, out) for (inp, out) in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92baf22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af59b9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        [6, 3, 1, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_featuresz=cat_features[:4]\n",
    "cat_featuresz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "050732d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.0838, -1.2799,  1.3361,  ...,  0.5473,  0.1226, -1.9235],\n",
       "         [-0.7965, -0.1908, -0.1844,  ...,  1.0819,  0.8208,  1.0587],\n",
       "         [ 1.0838, -1.2799,  1.3361,  ...,  0.5473,  0.1226, -1.9235],\n",
       "         ...,\n",
       "         [ 1.1349,  0.2774, -0.5403,  ...,  0.3591, -0.3699,  0.8540],\n",
       "         [-0.7965, -0.1908, -0.1844,  ...,  1.0819,  0.8208,  1.0587],\n",
       "         [-0.7965, -0.1908, -0.1844,  ...,  1.0819,  0.8208,  1.0587]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.4164, -1.7454,  1.5620],\n",
       "         [-0.4164, -1.7454,  1.5620],\n",
       "         [-0.4164, -1.7454,  1.5620],\n",
       "         ...,\n",
       "         [-0.4164, -1.7454,  1.5620],\n",
       "         [-0.4164, -1.7454,  1.5620],\n",
       "         [-0.4164, -1.7454,  1.5620]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[0.7022],\n",
       "         [0.7022],\n",
       "         [0.7022],\n",
       "         ...,\n",
       "         [0.7022],\n",
       "         [0.7022],\n",
       "         [0.7022]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-1.7378,  0.4700],\n",
       "         [-1.7378,  0.4700],\n",
       "         [ 1.3624,  0.8180],\n",
       "         ...,\n",
       "         [-1.7378,  0.4700],\n",
       "         [-1.7378,  0.4700],\n",
       "         [-1.7378,  0.4700]], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "embedding_val=[]\n",
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_features[:,i]))\n",
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0874be45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0838, -1.2799,  1.3361,  ...,  0.7022, -1.7378,  0.4700],\n",
       "        [-0.7965, -0.1908, -0.1844,  ...,  0.7022, -1.7378,  0.4700],\n",
       "        [ 1.0838, -1.2799,  1.3361,  ...,  0.7022,  1.3624,  0.8180],\n",
       "        ...,\n",
       "        [ 1.1349,  0.2774, -0.5403,  ...,  0.7022, -1.7378,  0.4700],\n",
       "        [-0.7965, -0.1908, -0.1844,  ...,  0.7022, -1.7378,  0.4700],\n",
       "        [-0.7965, -0.1908, -0.1844,  ...,  0.7022, -1.7378,  0.4700]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.cat(embedding_val, 1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5d881c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca9c0a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  2.2269,  ...,  0.0000, -2.8963,  0.7834],\n",
       "        [-0.0000, -0.3180, -0.3073,  ...,  1.1703, -2.8963,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  ...,  1.1703,  2.2707,  1.3633],\n",
       "        ...,\n",
       "        [ 1.8915,  0.4623, -0.9005,  ...,  1.1703, -0.0000,  0.7834],\n",
       "        [-0.0000, -0.0000, -0.3073,  ...,  1.1703, -0.0000,  0.0000],\n",
       "        [-1.3275, -0.0000, -0.0000,  ...,  1.1703, -2.8963,  0.7834]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed = dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ac310af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a feed forward neural network\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p= 0.4):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp, out) for (inp, out) in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((out for inp, out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in, i))\n",
    "            layerlist.append(nn.ReLU(inplace= True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1], out_sz))\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:, i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b629c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91551994",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = FeedForwardNN(embedding_dim, len(cont_features), 1, [100, 50], p= 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c28edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a343a1",
   "metadata": {},
   "source": [
    "### define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad1e1719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97506931",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss() #later will converted into Root mean squared error(RMSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c4aca55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09c2b9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,    21.,   856.,   854.],\n",
       "        [   80.,  9600.,    48.,  1262.,     0.],\n",
       "        [   68., 11250.,    23.,   920.,   866.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1941.,  1188.,  1152.],\n",
       "        [   68.,  9717.,  1950.,  1078.,     0.],\n",
       "        [   75.,  9937.,  1965.,  1256.,     0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "745da1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce80a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "batch_size = 1200\n",
    "test_size = int(batch_size * 0.15)\n",
    "train_categorical = cat_features[:batch_size - test_size]\n",
    "test_categorical = cat_features[batch_size - test_size : batch_size]\n",
    "train_cont = cont_values[:batch_size - test_size]\n",
    "test_cont = cont_values[batch_size - test_size : batch_size]\n",
    "y_train = y[:batch_size - test_size]\n",
    "y_test = y[batch_size - test_size : batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73b9a87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical), len(test_categorical), len(train_cont), len(test_cont), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d22bcace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:-----  1   _________  loss:-------  200496.765625\n",
      "epoch:-----  11   _________  loss:-------  200493.75\n",
      "epoch:-----  21   _________  loss:-------  200489.46875\n",
      "epoch:-----  31   _________  loss:-------  200483.03125\n",
      "epoch:-----  41   _________  loss:-------  200473.71875\n",
      "epoch:-----  51   _________  loss:-------  200462.09375\n",
      "epoch:-----  61   _________  loss:-------  200446.78125\n",
      "epoch:-----  71   _________  loss:-------  200430.09375\n",
      "epoch:-----  81   _________  loss:-------  200408.296875\n",
      "epoch:-----  91   _________  loss:-------  200384.390625\n",
      "epoch:-----  101   _________  loss:-------  200355.640625\n",
      "epoch:-----  111   _________  loss:-------  200322.4375\n",
      "epoch:-----  121   _________  loss:-------  200291.203125\n",
      "epoch:-----  131   _________  loss:-------  200251.984375\n",
      "epoch:-----  141   _________  loss:-------  200206.3125\n",
      "epoch:-----  151   _________  loss:-------  200161.359375\n",
      "epoch:-----  161   _________  loss:-------  200111.046875\n",
      "epoch:-----  171   _________  loss:-------  200058.84375\n",
      "epoch:-----  181   _________  loss:-------  200005.59375\n",
      "epoch:-----  191   _________  loss:-------  199946.46875\n",
      "epoch:-----  201   _________  loss:-------  199882.109375\n",
      "epoch:-----  211   _________  loss:-------  199816.125\n",
      "epoch:-----  221   _________  loss:-------  199738.5\n",
      "epoch:-----  231   _________  loss:-------  199669.234375\n",
      "epoch:-----  241   _________  loss:-------  199590.875\n",
      "epoch:-----  251   _________  loss:-------  199506.671875\n",
      "epoch:-----  261   _________  loss:-------  199413.59375\n",
      "epoch:-----  271   _________  loss:-------  199328.859375\n",
      "epoch:-----  281   _________  loss:-------  199248.234375\n",
      "epoch:-----  291   _________  loss:-------  199146.234375\n",
      "epoch:-----  301   _________  loss:-------  199027.734375\n",
      "epoch:-----  311   _________  loss:-------  198929.640625\n",
      "epoch:-----  321   _________  loss:-------  198847.640625\n",
      "epoch:-----  331   _________  loss:-------  198697.953125\n",
      "epoch:-----  341   _________  loss:-------  198606.359375\n",
      "epoch:-----  351   _________  loss:-------  198493.109375\n",
      "epoch:-----  361   _________  loss:-------  198386.421875\n",
      "epoch:-----  371   _________  loss:-------  198244.984375\n",
      "epoch:-----  381   _________  loss:-------  198105.296875\n",
      "epoch:-----  391   _________  loss:-------  198021.640625\n",
      "epoch:-----  401   _________  loss:-------  197870.171875\n",
      "epoch:-----  411   _________  loss:-------  197713.171875\n",
      "epoch:-----  421   _________  loss:-------  197600.78125\n",
      "epoch:-----  431   _________  loss:-------  197416.625\n",
      "epoch:-----  441   _________  loss:-------  197286.21875\n",
      "epoch:-----  451   _________  loss:-------  197170.71875\n",
      "epoch:-----  461   _________  loss:-------  196956.5\n",
      "epoch:-----  471   _________  loss:-------  196878.953125\n",
      "epoch:-----  481   _________  loss:-------  196694.9375\n",
      "epoch:-----  491   _________  loss:-------  196497.984375\n",
      "epoch:-----  501   _________  loss:-------  196437.515625\n",
      "epoch:-----  511   _________  loss:-------  196205.90625\n",
      "epoch:-----  521   _________  loss:-------  196045.296875\n",
      "epoch:-----  531   _________  loss:-------  195845.453125\n",
      "epoch:-----  541   _________  loss:-------  195666.0625\n",
      "epoch:-----  551   _________  loss:-------  195461.5625\n",
      "epoch:-----  561   _________  loss:-------  195285.171875\n",
      "epoch:-----  571   _________  loss:-------  195075.234375\n",
      "epoch:-----  581   _________  loss:-------  194834.40625\n",
      "epoch:-----  591   _________  loss:-------  194696.703125\n",
      "epoch:-----  601   _________  loss:-------  194513.125\n",
      "epoch:-----  611   _________  loss:-------  194276.34375\n",
      "epoch:-----  621   _________  loss:-------  194105.0625\n",
      "epoch:-----  631   _________  loss:-------  193828.578125\n",
      "epoch:-----  641   _________  loss:-------  193689.765625\n",
      "epoch:-----  651   _________  loss:-------  193500.03125\n",
      "epoch:-----  661   _________  loss:-------  193238.90625\n",
      "epoch:-----  671   _________  loss:-------  193061.421875\n",
      "epoch:-----  681   _________  loss:-------  192782.125\n",
      "epoch:-----  691   _________  loss:-------  192495.078125\n",
      "epoch:-----  701   _________  loss:-------  192298.5\n",
      "epoch:-----  711   _________  loss:-------  192196.40625\n",
      "epoch:-----  721   _________  loss:-------  191963.203125\n",
      "epoch:-----  731   _________  loss:-------  191634.328125\n",
      "epoch:-----  741   _________  loss:-------  191439.828125\n",
      "epoch:-----  751   _________  loss:-------  191209.234375\n",
      "epoch:-----  761   _________  loss:-------  190974.234375\n",
      "epoch:-----  771   _________  loss:-------  190753.515625\n",
      "epoch:-----  781   _________  loss:-------  190468.765625\n",
      "epoch:-----  791   _________  loss:-------  190292.25\n",
      "epoch:-----  801   _________  loss:-------  189868.953125\n",
      "epoch:-----  811   _________  loss:-------  189770.796875\n",
      "epoch:-----  821   _________  loss:-------  189660.4375\n",
      "epoch:-----  831   _________  loss:-------  189286.0625\n",
      "epoch:-----  841   _________  loss:-------  188952.1875\n",
      "epoch:-----  851   _________  loss:-------  188839.96875\n",
      "epoch:-----  861   _________  loss:-------  188408.46875\n",
      "epoch:-----  871   _________  loss:-------  188207.890625\n",
      "epoch:-----  881   _________  loss:-------  187889.34375\n",
      "epoch:-----  891   _________  loss:-------  187465.859375\n",
      "epoch:-----  901   _________  loss:-------  187440.046875\n",
      "epoch:-----  911   _________  loss:-------  187217.875\n",
      "epoch:-----  921   _________  loss:-------  186836.921875\n",
      "epoch:-----  931   _________  loss:-------  186369.46875\n",
      "epoch:-----  941   _________  loss:-------  186175.96875\n",
      "epoch:-----  951   _________  loss:-------  185947.625\n",
      "epoch:-----  961   _________  loss:-------  185528.734375\n",
      "epoch:-----  971   _________  loss:-------  185310.203125\n",
      "epoch:-----  981   _________  loss:-------  185163.65625\n",
      "epoch:-----  991   _________  loss:-------  184789.125\n",
      "epoch:-----  1001   _________  loss:-------  184335.921875\n",
      "epoch:-----  1011   _________  loss:-------  184010.015625\n",
      "epoch:-----  1021   _________  loss:-------  183827.5625\n",
      "epoch:-----  1031   _________  loss:-------  183378.5\n",
      "epoch:-----  1041   _________  loss:-------  183215.421875\n",
      "epoch:-----  1051   _________  loss:-------  182893.640625\n",
      "epoch:-----  1061   _________  loss:-------  182534.953125\n",
      "epoch:-----  1071   _________  loss:-------  182313.40625\n",
      "epoch:-----  1081   _________  loss:-------  181929.328125\n",
      "epoch:-----  1091   _________  loss:-------  181490.5\n",
      "epoch:-----  1101   _________  loss:-------  181310.015625\n",
      "epoch:-----  1111   _________  loss:-------  180799.15625\n",
      "epoch:-----  1121   _________  loss:-------  180495.515625\n",
      "epoch:-----  1131   _________  loss:-------  180432.015625\n",
      "epoch:-----  1141   _________  loss:-------  179946.109375\n",
      "epoch:-----  1151   _________  loss:-------  179509.015625\n",
      "epoch:-----  1161   _________  loss:-------  179332.96875\n",
      "epoch:-----  1171   _________  loss:-------  178925.46875\n",
      "epoch:-----  1181   _________  loss:-------  178518.0625\n",
      "epoch:-----  1191   _________  loss:-------  178120.015625\n",
      "epoch:-----  1201   _________  loss:-------  178108.28125\n",
      "epoch:-----  1211   _________  loss:-------  177481.25\n",
      "epoch:-----  1221   _________  loss:-------  177526.109375\n",
      "epoch:-----  1231   _________  loss:-------  176621.796875\n",
      "epoch:-----  1241   _________  loss:-------  176278.15625\n",
      "epoch:-----  1251   _________  loss:-------  175536.625\n",
      "epoch:-----  1261   _________  loss:-------  175561.09375\n",
      "epoch:-----  1271   _________  loss:-------  175250.625\n",
      "epoch:-----  1281   _________  loss:-------  174884.171875\n",
      "epoch:-----  1291   _________  loss:-------  174446.03125\n",
      "epoch:-----  1301   _________  loss:-------  174241.265625\n",
      "epoch:-----  1311   _________  loss:-------  173674.828125\n",
      "epoch:-----  1321   _________  loss:-------  173097.59375\n",
      "epoch:-----  1331   _________  loss:-------  173425.921875\n",
      "epoch:-----  1341   _________  loss:-------  172628.265625\n",
      "epoch:-----  1351   _________  loss:-------  172275.640625\n",
      "epoch:-----  1361   _________  loss:-------  172024.875\n",
      "epoch:-----  1371   _________  loss:-------  171850.109375\n",
      "epoch:-----  1381   _________  loss:-------  171306.0\n",
      "epoch:-----  1391   _________  loss:-------  170407.40625\n",
      "epoch:-----  1401   _________  loss:-------  170587.578125\n",
      "epoch:-----  1411   _________  loss:-------  169743.90625\n",
      "epoch:-----  1421   _________  loss:-------  169833.578125\n",
      "epoch:-----  1431   _________  loss:-------  169436.046875\n",
      "epoch:-----  1441   _________  loss:-------  168677.84375\n",
      "epoch:-----  1451   _________  loss:-------  168628.671875\n",
      "epoch:-----  1461   _________  loss:-------  168013.328125\n",
      "epoch:-----  1471   _________  loss:-------  167686.703125\n",
      "epoch:-----  1481   _________  loss:-------  167380.90625\n",
      "epoch:-----  1491   _________  loss:-------  166579.40625\n",
      "epoch:-----  1501   _________  loss:-------  166378.109375\n",
      "epoch:-----  1511   _________  loss:-------  165609.515625\n",
      "epoch:-----  1521   _________  loss:-------  165893.046875\n",
      "epoch:-----  1531   _________  loss:-------  165343.375\n",
      "epoch:-----  1541   _________  loss:-------  164690.984375\n",
      "epoch:-----  1551   _________  loss:-------  164532.96875\n",
      "epoch:-----  1561   _________  loss:-------  163796.640625\n",
      "epoch:-----  1571   _________  loss:-------  163537.203125\n",
      "epoch:-----  1581   _________  loss:-------  163125.25\n",
      "epoch:-----  1591   _________  loss:-------  163041.28125\n",
      "epoch:-----  1601   _________  loss:-------  162438.328125\n",
      "epoch:-----  1611   _________  loss:-------  161734.359375\n",
      "epoch:-----  1621   _________  loss:-------  160833.125\n",
      "epoch:-----  1631   _________  loss:-------  160867.0625\n",
      "epoch:-----  1641   _________  loss:-------  160709.328125\n",
      "epoch:-----  1651   _________  loss:-------  160183.546875\n",
      "epoch:-----  1661   _________  loss:-------  160172.625\n",
      "epoch:-----  1671   _________  loss:-------  159153.90625\n",
      "epoch:-----  1681   _________  loss:-------  158419.484375\n",
      "epoch:-----  1691   _________  loss:-------  158344.953125\n",
      "epoch:-----  1701   _________  loss:-------  157633.71875\n",
      "epoch:-----  1711   _________  loss:-------  157505.78125\n",
      "epoch:-----  1721   _________  loss:-------  157119.59375\n",
      "epoch:-----  1731   _________  loss:-------  156752.5\n",
      "epoch:-----  1741   _________  loss:-------  156375.640625\n",
      "epoch:-----  1751   _________  loss:-------  156205.484375\n",
      "epoch:-----  1761   _________  loss:-------  154861.28125\n",
      "epoch:-----  1771   _________  loss:-------  154370.859375\n",
      "epoch:-----  1781   _________  loss:-------  154130.46875\n",
      "epoch:-----  1791   _________  loss:-------  153931.828125\n",
      "epoch:-----  1801   _________  loss:-------  153215.46875\n",
      "epoch:-----  1811   _________  loss:-------  152958.078125\n",
      "epoch:-----  1821   _________  loss:-------  152288.96875\n",
      "epoch:-----  1831   _________  loss:-------  152501.796875\n",
      "epoch:-----  1841   _________  loss:-------  151767.765625\n",
      "epoch:-----  1851   _________  loss:-------  150604.734375\n",
      "epoch:-----  1861   _________  loss:-------  150706.0\n",
      "epoch:-----  1871   _________  loss:-------  150226.1875\n",
      "epoch:-----  1881   _________  loss:-------  149330.625\n",
      "epoch:-----  1891   _________  loss:-------  148522.859375\n",
      "epoch:-----  1901   _________  loss:-------  148768.671875\n",
      "epoch:-----  1911   _________  loss:-------  148987.328125\n",
      "epoch:-----  1921   _________  loss:-------  147799.828125\n",
      "epoch:-----  1931   _________  loss:-------  147396.84375\n",
      "epoch:-----  1941   _________  loss:-------  146752.921875\n",
      "epoch:-----  1951   _________  loss:-------  146620.953125\n",
      "epoch:-----  1961   _________  loss:-------  146155.03125\n",
      "epoch:-----  1971   _________  loss:-------  145614.046875\n",
      "epoch:-----  1981   _________  loss:-------  145874.609375\n",
      "epoch:-----  1991   _________  loss:-------  144565.140625\n",
      "epoch:-----  2001   _________  loss:-------  144183.953125\n",
      "epoch:-----  2011   _________  loss:-------  143109.828125\n",
      "epoch:-----  2021   _________  loss:-------  143084.84375\n",
      "epoch:-----  2031   _________  loss:-------  142568.265625\n",
      "epoch:-----  2041   _________  loss:-------  142220.90625\n",
      "epoch:-----  2051   _________  loss:-------  140984.578125\n",
      "epoch:-----  2061   _________  loss:-------  141173.1875\n",
      "epoch:-----  2071   _________  loss:-------  141201.953125\n",
      "epoch:-----  2081   _________  loss:-------  139729.3125\n",
      "epoch:-----  2091   _________  loss:-------  139822.484375\n",
      "epoch:-----  2101   _________  loss:-------  139446.21875\n",
      "epoch:-----  2111   _________  loss:-------  138797.515625\n",
      "epoch:-----  2121   _________  loss:-------  137757.875\n",
      "epoch:-----  2131   _________  loss:-------  138094.171875\n",
      "epoch:-----  2141   _________  loss:-------  137355.953125\n",
      "epoch:-----  2151   _________  loss:-------  136725.484375\n",
      "epoch:-----  2161   _________  loss:-------  136663.8125\n",
      "epoch:-----  2171   _________  loss:-------  136673.171875\n",
      "epoch:-----  2181   _________  loss:-------  135956.046875\n",
      "epoch:-----  2191   _________  loss:-------  134900.90625\n",
      "epoch:-----  2201   _________  loss:-------  134220.703125\n",
      "epoch:-----  2211   _________  loss:-------  133970.03125\n",
      "epoch:-----  2221   _________  loss:-------  132879.21875\n",
      "epoch:-----  2231   _________  loss:-------  132737.34375\n",
      "epoch:-----  2241   _________  loss:-------  131810.0\n",
      "epoch:-----  2251   _________  loss:-------  131766.15625\n",
      "epoch:-----  2261   _________  loss:-------  130202.046875\n",
      "epoch:-----  2271   _________  loss:-------  130405.8125\n",
      "epoch:-----  2281   _________  loss:-------  130304.0\n",
      "epoch:-----  2291   _________  loss:-------  129540.0234375\n",
      "epoch:-----  2301   _________  loss:-------  128948.1484375\n",
      "epoch:-----  2311   _________  loss:-------  129091.734375\n",
      "epoch:-----  2321   _________  loss:-------  128410.9296875\n",
      "epoch:-----  2331   _________  loss:-------  128621.5\n",
      "epoch:-----  2341   _________  loss:-------  127602.5625\n",
      "epoch:-----  2351   _________  loss:-------  126332.390625\n",
      "epoch:-----  2361   _________  loss:-------  126254.828125\n",
      "epoch:-----  2371   _________  loss:-------  125733.953125\n",
      "epoch:-----  2381   _________  loss:-------  124921.390625\n",
      "epoch:-----  2391   _________  loss:-------  124940.5078125\n",
      "epoch:-----  2401   _________  loss:-------  124440.1484375\n",
      "epoch:-----  2411   _________  loss:-------  123407.0546875\n",
      "epoch:-----  2421   _________  loss:-------  123584.0390625\n",
      "epoch:-----  2431   _________  loss:-------  122471.1015625\n",
      "epoch:-----  2441   _________  loss:-------  122687.984375\n",
      "epoch:-----  2451   _________  loss:-------  121613.5390625\n",
      "epoch:-----  2461   _________  loss:-------  120564.6953125\n",
      "epoch:-----  2471   _________  loss:-------  120845.484375\n",
      "epoch:-----  2481   _________  loss:-------  120621.1640625\n",
      "epoch:-----  2491   _________  loss:-------  120650.40625\n",
      "epoch:-----  2501   _________  loss:-------  119691.03125\n",
      "epoch:-----  2511   _________  loss:-------  118739.671875\n",
      "epoch:-----  2521   _________  loss:-------  117768.421875\n",
      "epoch:-----  2531   _________  loss:-------  118538.09375\n",
      "epoch:-----  2541   _________  loss:-------  117375.359375\n",
      "epoch:-----  2551   _________  loss:-------  117153.125\n",
      "epoch:-----  2561   _________  loss:-------  116407.984375\n",
      "epoch:-----  2571   _________  loss:-------  115811.71875\n",
      "epoch:-----  2581   _________  loss:-------  116060.2265625\n",
      "epoch:-----  2591   _________  loss:-------  115616.625\n",
      "epoch:-----  2601   _________  loss:-------  114591.890625\n",
      "epoch:-----  2611   _________  loss:-------  114531.7421875\n",
      "epoch:-----  2621   _________  loss:-------  114795.8203125\n",
      "epoch:-----  2631   _________  loss:-------  112882.1875\n",
      "epoch:-----  2641   _________  loss:-------  112155.6328125\n",
      "epoch:-----  2651   _________  loss:-------  112310.96875\n",
      "epoch:-----  2661   _________  loss:-------  111473.25\n",
      "epoch:-----  2671   _________  loss:-------  110954.84375\n",
      "epoch:-----  2681   _________  loss:-------  110154.3671875\n",
      "epoch:-----  2691   _________  loss:-------  109572.125\n",
      "epoch:-----  2701   _________  loss:-------  109849.46875\n",
      "epoch:-----  2711   _________  loss:-------  109369.4140625\n",
      "epoch:-----  2721   _________  loss:-------  109031.34375\n",
      "epoch:-----  2731   _________  loss:-------  108751.046875\n",
      "epoch:-----  2741   _________  loss:-------  107466.3515625\n",
      "epoch:-----  2751   _________  loss:-------  107795.21875\n",
      "epoch:-----  2761   _________  loss:-------  106059.265625\n",
      "epoch:-----  2771   _________  loss:-------  106898.71875\n",
      "epoch:-----  2781   _________  loss:-------  105499.9765625\n",
      "epoch:-----  2791   _________  loss:-------  104811.453125\n",
      "epoch:-----  2801   _________  loss:-------  103704.2265625\n",
      "epoch:-----  2811   _________  loss:-------  104000.5546875\n",
      "epoch:-----  2821   _________  loss:-------  102995.9140625\n",
      "epoch:-----  2831   _________  loss:-------  102866.140625\n",
      "epoch:-----  2841   _________  loss:-------  102704.1796875\n",
      "epoch:-----  2851   _________  loss:-------  101523.3984375\n",
      "epoch:-----  2861   _________  loss:-------  101985.2421875\n",
      "epoch:-----  2871   _________  loss:-------  100719.3203125\n",
      "epoch:-----  2881   _________  loss:-------  101225.7734375\n",
      "epoch:-----  2891   _________  loss:-------  101742.0390625\n",
      "epoch:-----  2901   _________  loss:-------  99011.328125\n",
      "epoch:-----  2911   _________  loss:-------  98517.7578125\n",
      "epoch:-----  2921   _________  loss:-------  98694.7421875\n",
      "epoch:-----  2931   _________  loss:-------  100028.9453125\n",
      "epoch:-----  2941   _________  loss:-------  97353.671875\n",
      "epoch:-----  2951   _________  loss:-------  97686.453125\n",
      "epoch:-----  2961   _________  loss:-------  96779.7265625\n",
      "epoch:-----  2971   _________  loss:-------  95526.375\n",
      "epoch:-----  2981   _________  loss:-------  95575.2109375\n",
      "epoch:-----  2991   _________  loss:-------  94712.8984375\n",
      "epoch:-----  3001   _________  loss:-------  94231.6953125\n",
      "epoch:-----  3011   _________  loss:-------  94517.0078125\n",
      "epoch:-----  3021   _________  loss:-------  93278.6015625\n",
      "epoch:-----  3031   _________  loss:-------  93049.6875\n",
      "epoch:-----  3041   _________  loss:-------  92891.1796875\n",
      "epoch:-----  3051   _________  loss:-------  93732.2734375\n",
      "epoch:-----  3061   _________  loss:-------  92372.3984375\n",
      "epoch:-----  3071   _________  loss:-------  90984.1015625\n",
      "epoch:-----  3081   _________  loss:-------  91272.6328125\n",
      "epoch:-----  3091   _________  loss:-------  90205.2109375\n",
      "epoch:-----  3101   _________  loss:-------  89326.0546875\n",
      "epoch:-----  3111   _________  loss:-------  89446.359375\n",
      "epoch:-----  3121   _________  loss:-------  89733.921875\n",
      "epoch:-----  3131   _________  loss:-------  88334.46875\n",
      "epoch:-----  3141   _________  loss:-------  88645.1015625\n",
      "epoch:-----  3151   _________  loss:-------  87889.671875\n",
      "epoch:-----  3161   _________  loss:-------  86476.8671875\n",
      "epoch:-----  3171   _________  loss:-------  87090.1328125\n",
      "epoch:-----  3181   _________  loss:-------  86245.671875\n",
      "epoch:-----  3191   _________  loss:-------  85682.8203125\n",
      "epoch:-----  3201   _________  loss:-------  85125.4453125\n",
      "epoch:-----  3211   _________  loss:-------  84916.078125\n",
      "epoch:-----  3221   _________  loss:-------  83474.3125\n",
      "epoch:-----  3231   _________  loss:-------  84402.6796875\n",
      "epoch:-----  3241   _________  loss:-------  82213.015625\n",
      "epoch:-----  3251   _________  loss:-------  81615.84375\n",
      "epoch:-----  3261   _________  loss:-------  81799.5546875\n",
      "epoch:-----  3271   _________  loss:-------  82019.0546875\n",
      "epoch:-----  3281   _________  loss:-------  81416.8671875\n",
      "epoch:-----  3291   _________  loss:-------  81049.203125\n",
      "epoch:-----  3301   _________  loss:-------  80173.9375\n",
      "epoch:-----  3311   _________  loss:-------  79802.53125\n",
      "epoch:-----  3321   _________  loss:-------  79618.234375\n",
      "epoch:-----  3331   _________  loss:-------  78857.5859375\n",
      "epoch:-----  3341   _________  loss:-------  77990.7421875\n",
      "epoch:-----  3351   _________  loss:-------  77659.9921875\n",
      "epoch:-----  3361   _________  loss:-------  76868.8046875\n",
      "epoch:-----  3371   _________  loss:-------  76688.4765625\n",
      "epoch:-----  3381   _________  loss:-------  76766.3828125\n",
      "epoch:-----  3391   _________  loss:-------  74677.328125\n",
      "epoch:-----  3401   _________  loss:-------  75079.6328125\n",
      "epoch:-----  3411   _________  loss:-------  74670.640625\n",
      "epoch:-----  3421   _________  loss:-------  74343.40625\n",
      "epoch:-----  3431   _________  loss:-------  74513.375\n",
      "epoch:-----  3441   _________  loss:-------  76480.3828125\n",
      "epoch:-----  3451   _________  loss:-------  75997.234375\n",
      "epoch:-----  3461   _________  loss:-------  73354.65625\n",
      "epoch:-----  3471   _________  loss:-------  71091.1875\n",
      "epoch:-----  3481   _________  loss:-------  70932.9609375\n",
      "epoch:-----  3491   _________  loss:-------  71489.5703125\n",
      "epoch:-----  3501   _________  loss:-------  69119.6953125\n",
      "epoch:-----  3511   _________  loss:-------  69918.890625\n",
      "epoch:-----  3521   _________  loss:-------  69173.296875\n",
      "epoch:-----  3531   _________  loss:-------  69056.2421875\n",
      "epoch:-----  3541   _________  loss:-------  69007.90625\n",
      "epoch:-----  3551   _________  loss:-------  68266.3515625\n",
      "epoch:-----  3561   _________  loss:-------  67395.3046875\n",
      "epoch:-----  3571   _________  loss:-------  66863.609375\n",
      "epoch:-----  3581   _________  loss:-------  66462.09375\n",
      "epoch:-----  3591   _________  loss:-------  67172.9921875\n",
      "epoch:-----  3601   _________  loss:-------  65564.8984375\n",
      "epoch:-----  3611   _________  loss:-------  66502.359375\n",
      "epoch:-----  3621   _________  loss:-------  66346.71875\n",
      "epoch:-----  3631   _________  loss:-------  63604.04296875\n",
      "epoch:-----  3641   _________  loss:-------  65251.18359375\n",
      "epoch:-----  3651   _________  loss:-------  62801.15234375\n",
      "epoch:-----  3661   _________  loss:-------  64458.73828125\n",
      "epoch:-----  3671   _________  loss:-------  61756.31640625\n",
      "epoch:-----  3681   _________  loss:-------  61694.5859375\n",
      "epoch:-----  3691   _________  loss:-------  61421.68359375\n",
      "epoch:-----  3701   _________  loss:-------  60881.9296875\n",
      "epoch:-----  3711   _________  loss:-------  62892.72265625\n",
      "epoch:-----  3721   _________  loss:-------  59811.37890625\n",
      "epoch:-----  3731   _________  loss:-------  57849.59765625\n",
      "epoch:-----  3741   _________  loss:-------  58653.36328125\n",
      "epoch:-----  3751   _________  loss:-------  59192.36328125\n",
      "epoch:-----  3761   _________  loss:-------  59270.51171875\n",
      "epoch:-----  3771   _________  loss:-------  57370.98828125\n",
      "epoch:-----  3781   _________  loss:-------  56299.61328125\n",
      "epoch:-----  3791   _________  loss:-------  56858.98046875\n",
      "epoch:-----  3801   _________  loss:-------  56333.08203125\n",
      "epoch:-----  3811   _________  loss:-------  56104.87109375\n",
      "epoch:-----  3821   _________  loss:-------  57313.015625\n",
      "epoch:-----  3831   _________  loss:-------  55042.2109375\n",
      "epoch:-----  3841   _________  loss:-------  53456.16015625\n",
      "epoch:-----  3851   _________  loss:-------  56138.14453125\n",
      "epoch:-----  3861   _________  loss:-------  54913.875\n",
      "epoch:-----  3871   _________  loss:-------  52148.171875\n",
      "epoch:-----  3881   _________  loss:-------  52245.5859375\n",
      "epoch:-----  3891   _________  loss:-------  51106.51953125\n",
      "epoch:-----  3901   _________  loss:-------  53343.56640625\n",
      "epoch:-----  3911   _________  loss:-------  52388.6953125\n",
      "epoch:-----  3921   _________  loss:-------  51683.28125\n",
      "epoch:-----  3931   _________  loss:-------  51034.7421875\n",
      "epoch:-----  3941   _________  loss:-------  50487.89453125\n",
      "epoch:-----  3951   _________  loss:-------  49250.2265625\n",
      "epoch:-----  3961   _________  loss:-------  50931.984375\n",
      "epoch:-----  3971   _________  loss:-------  49486.3515625\n",
      "epoch:-----  3981   _________  loss:-------  50465.5234375\n",
      "epoch:-----  3991   _________  loss:-------  49840.41015625\n",
      "epoch:-----  4001   _________  loss:-------  52137.015625\n",
      "epoch:-----  4011   _________  loss:-------  46679.03515625\n",
      "epoch:-----  4021   _________  loss:-------  49645.3359375\n",
      "epoch:-----  4031   _________  loss:-------  45468.3125\n",
      "epoch:-----  4041   _________  loss:-------  47466.19140625\n",
      "epoch:-----  4051   _________  loss:-------  48321.453125\n",
      "epoch:-----  4061   _________  loss:-------  45085.1796875\n",
      "epoch:-----  4071   _________  loss:-------  45844.875\n",
      "epoch:-----  4081   _________  loss:-------  44951.75390625\n",
      "epoch:-----  4091   _________  loss:-------  45794.90234375\n",
      "epoch:-----  4101   _________  loss:-------  44723.15234375\n",
      "epoch:-----  4111   _________  loss:-------  44762.671875\n",
      "epoch:-----  4121   _________  loss:-------  44347.1953125\n",
      "epoch:-----  4131   _________  loss:-------  43928.59375\n",
      "epoch:-----  4141   _________  loss:-------  44635.3046875\n",
      "epoch:-----  4151   _________  loss:-------  43805.63671875\n",
      "epoch:-----  4161   _________  loss:-------  43074.03515625\n",
      "epoch:-----  4171   _________  loss:-------  43024.16015625\n",
      "epoch:-----  4181   _________  loss:-------  42328.3046875\n",
      "epoch:-----  4191   _________  loss:-------  43338.3828125\n",
      "epoch:-----  4201   _________  loss:-------  42807.28125\n",
      "epoch:-----  4211   _________  loss:-------  40843.5078125\n",
      "epoch:-----  4221   _________  loss:-------  40253.43359375\n",
      "epoch:-----  4231   _________  loss:-------  41600.9140625\n",
      "epoch:-----  4241   _________  loss:-------  40980.00390625\n",
      "epoch:-----  4251   _________  loss:-------  41105.546875\n",
      "epoch:-----  4261   _________  loss:-------  41185.50390625\n",
      "epoch:-----  4271   _________  loss:-------  41132.6640625\n",
      "epoch:-----  4281   _________  loss:-------  42406.5703125\n",
      "epoch:-----  4291   _________  loss:-------  39753.71875\n",
      "epoch:-----  4301   _________  loss:-------  41076.78515625\n",
      "epoch:-----  4311   _________  loss:-------  39243.00390625\n",
      "epoch:-----  4321   _________  loss:-------  39877.34375\n",
      "epoch:-----  4331   _________  loss:-------  39814.7890625\n",
      "epoch:-----  4341   _________  loss:-------  39111.56640625\n",
      "epoch:-----  4351   _________  loss:-------  42137.46484375\n",
      "epoch:-----  4361   _________  loss:-------  40800.0234375\n",
      "epoch:-----  4371   _________  loss:-------  38685.59375\n",
      "epoch:-----  4381   _________  loss:-------  39284.20703125\n",
      "epoch:-----  4391   _________  loss:-------  37351.10546875\n",
      "epoch:-----  4401   _________  loss:-------  37355.8046875\n",
      "epoch:-----  4411   _________  loss:-------  39525.20703125\n",
      "epoch:-----  4421   _________  loss:-------  40198.4296875\n",
      "epoch:-----  4431   _________  loss:-------  38533.45703125\n",
      "epoch:-----  4441   _________  loss:-------  37968.93359375\n",
      "epoch:-----  4451   _________  loss:-------  37852.9921875\n",
      "epoch:-----  4461   _________  loss:-------  37459.3984375\n",
      "epoch:-----  4471   _________  loss:-------  38061.42578125\n",
      "epoch:-----  4481   _________  loss:-------  38135.5078125\n",
      "epoch:-----  4491   _________  loss:-------  36569.2265625\n",
      "epoch:-----  4501   _________  loss:-------  37406.52734375\n",
      "epoch:-----  4511   _________  loss:-------  36484.24609375\n",
      "epoch:-----  4521   _________  loss:-------  36859.375\n",
      "epoch:-----  4531   _________  loss:-------  37460.16015625\n",
      "epoch:-----  4541   _________  loss:-------  36006.23046875\n",
      "epoch:-----  4551   _________  loss:-------  36963.04296875\n",
      "epoch:-----  4561   _________  loss:-------  37016.00390625\n",
      "epoch:-----  4571   _________  loss:-------  37476.3359375\n",
      "epoch:-----  4581   _________  loss:-------  36248.10546875\n",
      "epoch:-----  4591   _________  loss:-------  34739.15234375\n",
      "epoch:-----  4601   _________  loss:-------  37394.89453125\n",
      "epoch:-----  4611   _________  loss:-------  35895.640625\n",
      "epoch:-----  4621   _________  loss:-------  36337.5234375\n",
      "epoch:-----  4631   _________  loss:-------  36357.21875\n",
      "epoch:-----  4641   _________  loss:-------  36406.18359375\n",
      "epoch:-----  4651   _________  loss:-------  36302.2578125\n",
      "epoch:-----  4661   _________  loss:-------  36162.55859375\n",
      "epoch:-----  4671   _________  loss:-------  37368.83203125\n",
      "epoch:-----  4681   _________  loss:-------  35691.62890625\n",
      "epoch:-----  4691   _________  loss:-------  38014.34765625\n",
      "epoch:-----  4701   _________  loss:-------  34674.81640625\n",
      "epoch:-----  4711   _________  loss:-------  34298.4921875\n",
      "epoch:-----  4721   _________  loss:-------  35266.20703125\n",
      "epoch:-----  4731   _________  loss:-------  35887.45703125\n",
      "epoch:-----  4741   _________  loss:-------  35295.234375\n",
      "epoch:-----  4751   _________  loss:-------  36688.92578125\n",
      "epoch:-----  4761   _________  loss:-------  35262.64453125\n",
      "epoch:-----  4771   _________  loss:-------  36029.1171875\n",
      "epoch:-----  4781   _________  loss:-------  33507.69140625\n",
      "epoch:-----  4791   _________  loss:-------  36118.02734375\n",
      "epoch:-----  4801   _________  loss:-------  35264.19140625\n",
      "epoch:-----  4811   _________  loss:-------  35923.9921875\n",
      "epoch:-----  4821   _________  loss:-------  35055.51953125\n",
      "epoch:-----  4831   _________  loss:-------  36014.16015625\n",
      "epoch:-----  4841   _________  loss:-------  35651.671875\n",
      "epoch:-----  4851   _________  loss:-------  37242.98046875\n",
      "epoch:-----  4861   _________  loss:-------  35030.83203125\n",
      "epoch:-----  4871   _________  loss:-------  36157.1640625\n",
      "epoch:-----  4881   _________  loss:-------  36232.04296875\n",
      "epoch:-----  4891   _________  loss:-------  35747.12109375\n",
      "epoch:-----  4901   _________  loss:-------  36759.59765625\n",
      "epoch:-----  4911   _________  loss:-------  35426.6484375\n",
      "epoch:-----  4921   _________  loss:-------  35126.89453125\n",
      "epoch:-----  4931   _________  loss:-------  36150.28125\n",
      "epoch:-----  4941   _________  loss:-------  36803.5234375\n",
      "epoch:-----  4951   _________  loss:-------  35497.51171875\n",
      "epoch:-----  4961   _________  loss:-------  35903.4140625\n",
      "epoch:-----  4971   _________  loss:-------  36130.609375\n",
      "epoch:-----  4981   _________  loss:-------  35172.3125\n",
      "epoch:-----  4991   _________  loss:-------  34117.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:-----  4281   _________  loss:-------  43921.734375\n",
      "epoch:-----  4291   _________  loss:-------  39902.09375\n",
      "epoch:-----  4301   _________  loss:-------  41520.46484375\n",
      "epoch:-----  4311   _________  loss:-------  40581.328125\n",
      "epoch:-----  4321   _________  loss:-------  40318.30078125\n",
      "epoch:-----  4331   _________  loss:-------  40091.4296875\n",
      "epoch:-----  4341   _________  loss:-------  39962.50390625\n",
      "epoch:-----  4351   _________  loss:-------  43013.19140625\n",
      "epoch:-----  4361   _________  loss:-------  41675.36328125\n",
      "epoch:-----  4371   _________  loss:-------  39120.09765625\n",
      "epoch:-----  4381   _________  loss:-------  38906.20703125\n",
      "epoch:-----  4391   _________  loss:-------  38524.265625\n",
      "epoch:-----  4401   _________  loss:-------  38115.37109375\n",
      "epoch:-----  4411   _________  loss:-------  39240.1640625\n",
      "epoch:-----  4421   _________  loss:-------  40091.421875\n",
      "epoch:-----  4431   _________  loss:-------  38793.46484375\n",
      "epoch:-----  4441   _________  loss:-------  38367.703125\n",
      "epoch:-----  4451   _________  loss:-------  38451.11328125\n",
      "epoch:-----  4461   _________  loss:-------  38095.28125\n",
      "epoch:-----  4471   _________  loss:-------  38771.875\n",
      "epoch:-----  4481   _________  loss:-------  40577.16015625\n",
      "epoch:-----  4491   _________  loss:-------  38577.640625\n",
      "epoch:-----  4501   _________  loss:-------  37887.62890625\n",
      "epoch:-----  4511   _________  loss:-------  37889.5859375\n",
      "epoch:-----  4521   _________  loss:-------  37472.93359375\n",
      "epoch:-----  4531   _________  loss:-------  38226.26171875\n",
      "epoch:-----  4541   _________  loss:-------  36126.78515625\n",
      "epoch:-----  4551   _________  loss:-------  35917.6015625\n",
      "epoch:-----  4561   _________  loss:-------  37483.703125\n",
      "epoch:-----  4571   _________  loss:-------  37068.37109375\n",
      "epoch:-----  4581   _________  loss:-------  36432.5859375\n",
      "epoch:-----  4591   _________  loss:-------  35427.734375\n",
      "epoch:-----  4601   _________  loss:-------  37893.08203125\n",
      "epoch:-----  4611   _________  loss:-------  36267.484375\n",
      "epoch:-----  4621   _________  loss:-------  37886.9453125\n",
      "epoch:-----  4631   _________  loss:-------  37623.67578125\n",
      "epoch:-----  4641   _________  loss:-------  36028.5625\n",
      "epoch:-----  4651   _________  loss:-------  36399.53515625\n",
      "epoch:-----  4661   _________  loss:-------  36474.22265625\n",
      "epoch:-----  4671   _________  loss:-------  38280.34765625\n",
      "epoch:-----  4681   _________  loss:-------  36862.3046875\n",
      "epoch:-----  4691   _________  loss:-------  38504.36328125\n",
      "epoch:-----  4701   _________  loss:-------  36259.7734375\n",
      "epoch:-----  4711   _________  loss:-------  34719.4453125\n",
      "epoch:-----  4721   _________  loss:-------  35047.296875\n",
      "epoch:-----  4731   _________  loss:-------  36496.8671875\n",
      "epoch:-----  4741   _________  loss:-------  35405.88671875\n",
      "epoch:-----  4751   _________  loss:-------  38281.07421875\n",
      "epoch:-----  4761   _________  loss:-------  36661.6328125\n",
      "epoch:-----  4771   _________  loss:-------  36209.33203125\n",
      "epoch:-----  4781   _________  loss:-------  34400.01953125\n",
      "epoch:-----  4791   _________  loss:-------  36772.96484375\n",
      "epoch:-----  4801   _________  loss:-------  36229.34765625\n",
      "epoch:-----  4811   _________  loss:-------  36928.3046875\n",
      "epoch:-----  4821   _________  loss:-------  35924.9921875\n",
      "epoch:-----  4831   _________  loss:-------  37517.73828125\n",
      "epoch:-----  4841   _________  loss:-------  36837.34765625\n",
      "epoch:-----  4851   _________  loss:-------  37629.48828125\n",
      "epoch:-----  4861   _________  loss:-------  35839.90234375\n",
      "epoch:-----  4871   _________  loss:-------  36302.39453125\n",
      "epoch:-----  4881   _________  loss:-------  37071.85546875\n",
      "epoch:-----  4891   _________  loss:-------  35141.18359375\n",
      "epoch:-----  4901   _________  loss:-------  36707.83203125\n",
      "epoch:-----  4911   _________  loss:-------  35290.40625\n",
      "epoch:-----  4921   _________  loss:-------  35804.53515625\n",
      "epoch:-----  4931   _________  loss:-------  37380.44140625\n",
      "epoch:-----  4941   _________  loss:-------  35659.3515625\n",
      "epoch:-----  4951   _________  loss:-------  35238.2265625\n",
      "epoch:-----  4961   _________  loss:-------  34833.3984375\n",
      "epoch:-----  4971   _________  loss:-------  36682.73046875\n",
      "epoch:-----  4981   _________  loss:-------  36073.54296875\n",
      "epoch:-----  4991   _________  loss:-------  34649.421875\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "final_losses = []\n",
    "for i in range(epochs):\n",
    "    i = i + 1\n",
    "    y_pred = model(train_categorical, train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred, y_train))# RMSE\n",
    "    final_losses.append(loss)\n",
    "    if i % 10 == 1:\n",
    "        print('epoch:-----  {}   _________  loss:-------  {}'.format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a9e6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmpy = torch.tensor(final_losses).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "155ea3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(epochs), nmpy)\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5d22505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 56874.49609375\n"
     ]
    }
   ],
   "source": [
    "# validate the test data\n",
    "y_pred = ''\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_categorical, test_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred, y_test))\n",
    "print('RMSE: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7c70066",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=[\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbe7b8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134603.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177940.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243667.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>301599.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>231548.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>194346.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>225998.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114874.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>210178.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>192363.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>232805.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>151625.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>98621.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>183647.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>112843.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>309189.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>145694.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>332931.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>94737.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>213385.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>124571.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>90634.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171770.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>235056.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>248037.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>151799.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>246499.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>143309.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>136878.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>109869.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>111790.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>325095.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>78488.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>243061.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>219466.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>107619.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>111261.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>155045.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>70533.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>276200.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>300583.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>79958.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>182664.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>195038.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>86018.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>88493.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>129461.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>200050.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>200511.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>122527.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>348781.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>238023.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>228575.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>190244.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>191178.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>174981.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>111871.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>208419.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>243181.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>327502.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>113717.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>219790.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>215691.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>172338.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>124620.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>152773.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>331608.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>97954.476562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>169915.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>106614.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>102147.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>98121.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>216886.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>265590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>221492.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>148915.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>116372.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>85930.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>86636.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>119880.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>97273.992188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>75106.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>266214.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>106475.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>92412.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>234942.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>167056.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>191127.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>101079.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>205282.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>134111.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>119966.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>128466.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>113038.414062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>414509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>206202.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>225352.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>179132.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>231128.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>248106.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>300904.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>300975.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>96280.804688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>306372.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>144090.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>132796.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>292240.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>284500.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>200014.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>112381.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>103354.476562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>62054.949219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>224992.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>119502.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>113659.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>148432.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>106064.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>319598.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>238241.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>231897.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>94029.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>211901.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>88298.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>104691.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>136914.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>233287.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>267834.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>87730.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>148045.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>106284.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>103168.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>255216.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>245031.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>303696.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>112440.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>219207.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>115715.992188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>147846.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>256296.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>281430.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>150596.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>83796.226562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>243141.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>147381.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>220925.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>105058.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>142458.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>139922.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>106088.382812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>191422.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>128256.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>259848.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>120075.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>116975.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>240618.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>129666.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>259851.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>143795.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>138946.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>125550.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>202644.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>118315.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>198714.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>148046.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>330686.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>228120.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>83227.242188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>297027.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>137188.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>102457.054688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>102335.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>233080.453125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>160833.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>209537.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>219954.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>228084.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>228181.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>186120.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>114756.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prediction\n",
       "0    134603.171875\n",
       "1    177940.640625\n",
       "2    243667.468750\n",
       "3    287231.000000\n",
       "4    301599.500000\n",
       "5    231548.953125\n",
       "6    194346.984375\n",
       "7    225998.328125\n",
       "8    114874.304688\n",
       "9    210178.828125\n",
       "10   192363.656250\n",
       "11   232805.796875\n",
       "12   151625.046875\n",
       "13    98621.070312\n",
       "14   183647.625000\n",
       "15   112843.710938\n",
       "16   309189.406250\n",
       "17   145694.031250\n",
       "18   332931.625000\n",
       "19    94737.820312\n",
       "20   213385.921875\n",
       "21   124571.523438\n",
       "22    90634.976562\n",
       "23   171770.828125\n",
       "24   235056.953125\n",
       "25   248037.359375\n",
       "26   151799.843750\n",
       "27   246499.062500\n",
       "28   143309.250000\n",
       "29   136878.468750\n",
       "30   109869.812500\n",
       "31   111790.648438\n",
       "32   325095.531250\n",
       "33    78488.429688\n",
       "34   243061.437500\n",
       "35   219466.171875\n",
       "36   107619.710938\n",
       "37   111261.679688\n",
       "38   155045.796875\n",
       "39    70533.281250\n",
       "40   276200.125000\n",
       "41   300583.406250\n",
       "42    79958.726562\n",
       "43   182664.750000\n",
       "44   195038.156250\n",
       "45    86018.234375\n",
       "46    88493.554688\n",
       "47   129461.882812\n",
       "48   200050.546875\n",
       "49   200511.234375\n",
       "50   122527.960938\n",
       "51   348781.843750\n",
       "52   238023.390625\n",
       "53   228575.296875\n",
       "54   190244.078125\n",
       "55   191178.437500\n",
       "56   174981.156250\n",
       "57   111871.062500\n",
       "58   208419.609375\n",
       "59   243181.171875\n",
       "60   327502.093750\n",
       "61   113717.570312\n",
       "62   219790.468750\n",
       "63   215691.140625\n",
       "64   172338.171875\n",
       "65   124620.625000\n",
       "66   152773.109375\n",
       "67   331608.062500\n",
       "68    97954.476562\n",
       "69   169915.578125\n",
       "70   106614.406250\n",
       "71   102147.695312\n",
       "72    98121.523438\n",
       "73   216886.281250\n",
       "74   265590.000000\n",
       "75   221492.953125\n",
       "76   148915.703125\n",
       "77   116372.320312\n",
       "78    85930.273438\n",
       "79    86636.289062\n",
       "80   119880.617188\n",
       "81    97273.992188\n",
       "82    75106.351562\n",
       "83   266214.218750\n",
       "84   106475.703125\n",
       "85    92412.867188\n",
       "86   234942.468750\n",
       "87   167056.812500\n",
       "88   191127.640625\n",
       "89   101079.523438\n",
       "90   205282.859375\n",
       "91   134111.078125\n",
       "92   119966.820312\n",
       "93   128466.148438\n",
       "94   113038.414062\n",
       "95   414509.000000\n",
       "96   206202.312500\n",
       "97   225352.828125\n",
       "98   179132.875000\n",
       "99   231128.578125\n",
       "100  248106.562500\n",
       "101  300904.468750\n",
       "102  300975.843750\n",
       "103   96280.804688\n",
       "104  306372.281250\n",
       "105  144090.609375\n",
       "106  132796.328125\n",
       "107  292240.906250\n",
       "108  284500.281250\n",
       "109  200014.562500\n",
       "110  112381.023438\n",
       "111  103354.476562\n",
       "112   62054.949219\n",
       "113  224992.671875\n",
       "114  119502.570312\n",
       "115  113659.820312\n",
       "116  148432.593750\n",
       "117  106064.507812\n",
       "118  319598.281250\n",
       "119  238241.281250\n",
       "120  231897.968750\n",
       "121   94029.796875\n",
       "122  211901.109375\n",
       "123   88298.304688\n",
       "124  104691.781250\n",
       "125  136914.734375\n",
       "126  233287.484375\n",
       "127  267834.125000\n",
       "128   87730.304688\n",
       "129  148045.406250\n",
       "130  106284.609375\n",
       "131  103168.585938\n",
       "132  255216.750000\n",
       "133  245031.515625\n",
       "134  303696.625000\n",
       "135  112440.703125\n",
       "136  219207.187500\n",
       "137  115715.992188\n",
       "138  147846.125000\n",
       "139  256296.453125\n",
       "140  281430.156250\n",
       "141  150596.625000\n",
       "142   83796.226562\n",
       "143  243141.359375\n",
       "144  147381.609375\n",
       "145  220925.609375\n",
       "146  105058.195312\n",
       "147  142458.328125\n",
       "148  139922.515625\n",
       "149  106088.382812\n",
       "150  191422.734375\n",
       "151  128256.859375\n",
       "152  259848.375000\n",
       "153  120075.648438\n",
       "154  116975.195312\n",
       "155  240618.890625\n",
       "156  129666.726562\n",
       "157  259851.921875\n",
       "158  143795.234375\n",
       "159  138946.203125\n",
       "160  125550.593750\n",
       "161  202644.390625\n",
       "162  118315.320312\n",
       "163  198714.546875\n",
       "164  148046.140625\n",
       "165  330686.312500\n",
       "166  228120.875000\n",
       "167   83227.242188\n",
       "168  297027.562500\n",
       "169  137188.234375\n",
       "170  102457.054688\n",
       "171  102335.171875\n",
       "172  233080.453125\n",
       "173  160833.359375\n",
       "174  209537.781250\n",
       "175  219954.906250\n",
       "176  228084.546875\n",
       "177  228181.781250\n",
       "178  186120.578125\n",
       "179  114756.875000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])\n",
    "data_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d3b1431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>134603.171875</td>\n",
       "      <td>-4603.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>177940.640625</td>\n",
       "      <td>-39053.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>243667.468750</td>\n",
       "      <td>-68167.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>287231.000000</td>\n",
       "      <td>-92231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>301599.500000</td>\n",
       "      <td>-159099.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test     Prediction     Difference\n",
       "0  130000.0  134603.171875   -4603.171875\n",
       "1  138887.0  177940.640625  -39053.640625\n",
       "2  175500.0  243667.468750  -68167.468750\n",
       "3  195000.0  287231.000000  -92231.000000\n",
       "4  142500.0  301599.500000 -159099.500000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
